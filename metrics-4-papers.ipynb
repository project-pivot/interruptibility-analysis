{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Tockler data\n",
    "\n",
    "The input file is an Excel document that contains the following columns directly exported from Tockler: App, Type, Title, Begin, End.\n",
    "It also contains an addition column called 'Activity' with the label of the project the entry corresponds to or with 'Interruption' if it corresponds to any other activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tockler_data = pd.read_excel(\"tockler_log.xlsx\")\n",
    "tockler_data['Begin'] = pd.to_datetime(tockler_data['Begin'].dt.tz_localize('Europe/Amsterdam'))\n",
    "tockler_data['End'] = pd.to_datetime(tockler_data['End'].dt.tz_localize('Europe/Amsterdam'))\n",
    "tockler_data['Source'] = 'Tockler'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tockler_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge location data\n",
    "\n",
    "We assume that folder_path points to a folder that contains the JSON files with the location information as exported from Google Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_location_to_df(json_data):\n",
    "    data= {\n",
    "    \"Source\": [],\n",
    "    \"Begin\": [],\n",
    "    \"End\": [],\n",
    "    \"Activity\": [],\n",
    "    \"Title\": [],\n",
    "    \"Location\": [],\n",
    "    \"Duration\": [],\n",
    "    }\n",
    "\n",
    "    conv = []    \n",
    "\n",
    "    ls_actividades=json_data['timelineObjects']\n",
    "    for act in ls_actividades:\n",
    "        if 'activitySegment' in act:\n",
    "            valores=act['activitySegment']\n",
    "            fecha_inicio=pd.to_datetime(valores['duration']['startTimestamp'])\n",
    "            fecha_fin=pd.to_datetime(valores['duration']['endTimestamp'])\n",
    "            ubicacion=valores['activityType']\n",
    "        else:\n",
    "            valores = act['placeVisit']\n",
    "            fecha_inicio=pd.to_datetime(valores['duration']['startTimestamp'], utc=True)\n",
    "            fecha_fin=pd.to_datetime(valores['duration']['endTimestamp'], utc=True)\n",
    "            location = valores['location']\n",
    "            if 'name' in location:\n",
    "                ubicacion = location['name']\n",
    "            else:\n",
    "                ubicacion = location['address']\n",
    "\n",
    "        conv.append({\n",
    "            'Source':'Google Location', \n",
    "            'Begin':fecha_inicio,\n",
    "            'End':fecha_fin, \n",
    "            'Activity':'',\n",
    "            'Title':'', \n",
    "            'Location':ubicacion,\n",
    "            'Duration':pd.to_datetime(fecha_fin)-pd.to_datetime(fecha_inicio)\n",
    "        }) \n",
    "\n",
    "    return pd.DataFrame(conv)\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "folder_path = 'Location data'\n",
    "\n",
    "all_locations = []\n",
    "for f in os.listdir(folder_path):\n",
    "     full_path = os.path.join(folder_path, f)\n",
    "     if os.path.isfile(full_path):\n",
    "          print(f\"Processing {f}\")\n",
    "          with open(full_path) as file_open:\n",
    "              json_data = json.load(file_open)\n",
    "\n",
    "          all_locations.append(from_location_to_df(json_data))\n",
    "\n",
    "df_location = pd.concat(all_locations).sort_values(\"Begin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([tockler_data, df_location]).sort_values(\"Begin\").reset_index(drop=True)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_group = (~pd.isna(merged['Location'])).cumsum()\n",
    "new_location = merged.groupby(locations_group)['Location'].transform('first')\n",
    "location_start = merged.groupby(locations_group)['Begin'].transform('first')\n",
    "location_end = merged.groupby(locations_group)['End'].transform('first')\n",
    "apply_if = (location_start <= merged['Begin']) & (merged['Begin'] <= location_end)\n",
    "merged.loc[apply_if, 'Location'] = new_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dto = merged[merged['Source'] == 'Tockler'].copy().reset_index(drop=True)\n",
    "dto['Begin'] = pd.to_datetime(dto['Begin'])\n",
    "dto['End'] = pd.to_datetime(dto['End'])\n",
    "dto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dto[\"Location\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We discard all locations whose only recorded windows are NATIVE / NO_TITLE because they are likely to be incorrectly recorded\n",
    "discard_location = (dto[\"App\"] == \"NATIVE\").groupby(dto[\"Location\"]).all()\n",
    "locations_discarded = discard_location[discard_location].index.values\n",
    "locations_discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dto = dto.drop(dto[dto[\"Location\"].isin(locations_discarded)].index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the timeout to consider the inactivity time in the log\n",
    "INACTIVITY_THRESHOLD = pd.Timedelta(\"1m\")\n",
    "\n",
    "# We consider it a change if the activity is different or it is a new day or if the gap between the end of an activity and the beginning of the next is greater than a threshold\n",
    "change = (((dto[\"Activity\"].shift() != dto[\"Activity\"])) | (dto[\"Begin\"].shift().dt.day != dto[\"End\"].dt.day) | ((dto[\"Begin\"] - dto[\"End\"].shift()) > INACTIVITY_THRESHOLD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = change.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = dto.groupby(by=it).agg({\"Begin\": \"first\", \"End\": \"last\", \"Activity\": \"first\", \"Title\":\"first\", \"Source\":\"count\", \"Location\": \"first\"})\n",
    "pr[\"Duration\"] = pr[\"End\"] - pr[\"Begin\"]\n",
    "pr[\"Begin\"] = pd.to_datetime(pr[\"Begin\"])\n",
    "pr[\"End\"] = pd.to_datetime(pr[\"End\"])\n",
    "pr[\"Duration_minutes\"] = pr[\"Duration\"] / pd.Timedelta('1m')\n",
    "prev = pr[\"Activity\"].shift()\n",
    "prev.loc[pr[\"Begin\"].dt.date != pr[\"Begin\"].shift().dt.date] = np.nan\n",
    "pr[\"Prev\"] = prev\n",
    "pr[\"OffComputer\"] = ((pr[\"Begin\"] - pr.shift()[\"End\"] > INACTIVITY_THRESHOLD) & (pr[\"Begin\"].dt.day == pr.shift()[\"End\"].dt.day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, each of the rows of pr represent a time slot of uninterrupted time either by electronic interruptions \n",
    "# that are included in the log or by physical or other types of interruptions, which are not included in the log\n",
    "pr.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â We keep in projs only those activities that relate to the projects under analysis\n",
    "projs = pr[pr[\"Activity\"] != \"Interruption\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our definition of within-day interruption. It is the time between the end of a project activity and the beginning of a project activity that occurs within the same natural day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interruption_time_b = projs.groupby(\"Activity\").apply(lambda x: x[\"Begin\"]- x[\"End\"].shift(), include_groups=False)\n",
    "interruption_time_b.loc[projs.groupby(\"Activity\").apply(lambda x: x[\"Begin\"].dt.day != x[\"End\"].shift().dt.day, include_groups=False)] = np.nan\n",
    "projs[\"interruption_time\"] = interruption_time_b.reset_index(level=0, drop=True) / pd.Timedelta('1m')\n",
    "projs.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is how we prepare a dataset that uses days as unit instead of activities. The reason behind this is that we do not consider an interruption the night of each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdaily = projs.groupby([\"Activity\", projs[\"Begin\"].dt.date]).agg(Duration=('Duration', \"sum\"), Times=('Duration', \"count\")).reset_index().sort_values([\"Activity\", \"Begin\"])\n",
    "pdaily[\"Duration\"] = pdaily[\"Duration\"] / pd.Timedelta('1h')\n",
    "pdaily[\"Times\"] = pdaily[\"Times\"] - 1\n",
    "pdaily[\"Begin\"] = pd.to_datetime(pdaily[\"Begin\"]).dt.tz_localize('Europe/Amsterdam')\n",
    "pdaily[\"Gap\"] = pdaily.groupby('Activity')[\"Begin\"].transform(lambda x: (x - x.shift())/pd.Timedelta('1d'))\n",
    "pdaily.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projs.groupby(\"Activity\")[\"Begin\"].agg([\"first\", \"last\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(freq = None, grouper = None, interruption_threshold=None):\n",
    "    '''\n",
    "    This function computes the metrics using the values of projs and pdaily.\n",
    "\n",
    "    - freq refers to the temporal frequency used to compute the metrics (e.g. 1 week)\n",
    "    - grouper refers to an optional attribute used to group the metrics\n",
    "    - interruption_threshold refers to the threshold used to determine what an long interruption\n",
    "    is (default = 60 minutes)\n",
    "\n",
    "    '''\n",
    "    groupby_spec = [\"Activity\"]\n",
    "    if freq is not None:\n",
    "        groupby_spec = groupby_spec + [pd.Grouper(key=\"Begin\", freq=freq)]\n",
    "    if grouper is not None:\n",
    "        groupby_spec = groupby_spec + [grouper]\n",
    "    if interruption_threshold is None:\n",
    "        interruption_threshold = pd.Timedelta('60m') / pd.Timedelta('1m')\n",
    "\n",
    "    g = projs.groupby(groupby_spec).agg(SlotDuration=(\"Duration_minutes\", \"mean\"), InterruptionDuration=(\"interruption_time\", \"mean\"), OffComputerInterruption=(\"OffComputer\", \"sum\"))\n",
    "    d = pdaily.groupby(groupby_spec).agg(TimesResumed=(\"Times\", \"sum\"), TotalDurationHours=(\"Duration\", \"sum\"), DaysGap=(\"Gap\", \"mean\"))\n",
    "    d[\"InterruptionsPerWorkHour\"] = d[\"TimesResumed\"] / d[\"TotalDurationHours\"]\n",
    "    if freq is not None:\n",
    "        g2 = projs[projs[\"interruption_time\"] < interruption_threshold].groupby(by=[\"Activity\", pd.Grouper(key=\"Begin\", freq=freq)])[\"interruption_time\"].agg(ShortInterruptionDuration=\"mean\", NumShortInterruption=\"count\")\n",
    "        g3 = projs[projs[\"interruption_time\"] >= interruption_threshold].groupby([\"Activity\", pd.Grouper(key=\"Begin\", freq=freq)])[\"interruption_time\"].agg(LongInterruptionDuration=\"mean\", NumLongInterruption=\"count\")\n",
    "        num_activities = pdaily.groupby(pd.Grouper(key=\"Begin\", freq=freq))[\"Activity\"].nunique().rename(\"ActivityVariety\")\n",
    "        return pd.merge(pd.concat([g, g2, g3, d], axis=1), num_activities, left_on=\"Begin\", right_index=True, how=\"left\")\n",
    "    else:\n",
    "        g2 = projs[projs[\"interruption_time\"] < interruption_threshold].groupby(by=\"Activity\")[\"interruption_time\"].agg(ShortInterruptionDuration=\"mean\", NumShortInterruption=\"count\")\n",
    "        g3 = projs[projs[\"interruption_time\"] >= interruption_threshold].groupby(\"Activity\")[\"interruption_time\"].agg(LongInterruptionDuration=\"mean\", NumLongInterruption=\"count\")\n",
    "        return pd.concat([g, g2, g3, d], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_metrics = compute_metrics(freq='D')\n",
    "daily_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calendar\n",
    "\n",
    "The calendar data is a CSV as exported by Microsoft Outlook in Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_csv = pd.read_csv(\"calendar_data.CSV\", delimiter=',')\n",
    "calendar_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dto[\"Location\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel = [\"FLYING\"]\n",
    "df_location[((df_location[\"Location\"].isin(travel))).shift(1, fill_value=False) | (df_location[\"Location\"].str.contains(\"Airport\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a dict with the location and the country of the location\n",
    "\n",
    "country = {\n",
    "    \"location1\": \"country1\",\n",
    "    \"location2\": \"country1\",\n",
    "    \"location3\": \"country2\"\n",
    "}    \n",
    "home_country = \"ZXXX\"\n",
    "abroad = [c for c in country if country[c]!=home_country]\n",
    "travel = dto[\"Location\"].isin(abroad)\n",
    "location_change = dto.groupby((travel != travel.shift()).cumsum()).agg({\"Begin\": \"first\", \"End\":\"last\", \"Location\":\"first\"})\n",
    "location_change[location_change[\"Location\"].isin(abroad)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a manual merge of the travels recognized as being flying or at the airport and the work abroad based on the output obtained from the previous cells\n",
    "\n",
    "trip_dates = [\n",
    "    pd.Timestamp(year=2023, month=1, day=2, tz=\"Europe/Amsterdam\"),\n",
    "    pd.Timestamp(year=2023, month=1, day=26, tz=\"Europe/Amsterdam\"),\n",
    "    pd.Timestamp(year=2023, month=3, day=25, tz=\"Europe/Amsterdam\"),\n",
    "    pd.Timestamp(year=2023, month=4, day=28, tz=\"Europe/Amsterdam\"),\n",
    "    pd.Timestamp(year=2023, month=5, day=23, tz=\"Europe/Amsterdam\"),\n",
    "    pd.Timestamp(year=2023, month=6, day=10, tz=\"Europe/Amsterdam\"),\n",
    "    pd.Timestamp(year=2023, month=7, day=7, tz=\"Europe/Amsterdam\")\n",
    "]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping location to home / work / other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a map where you specify the location and whether this location should be considered Home / Work / Other\n",
    "\n",
    "location_map = {\n",
    "    \"location1\": \"Home\",\n",
    "    \"location2\": \"Work\",\n",
    "    \"location3\": \"Home\"\n",
    "}\n",
    "\n",
    "location_values = dto[\"Location\"].dropna().unique()\n",
    "location_mapped = dict()\n",
    "for l in location_values:\n",
    "    location_mapped[l] = location_map[l] if l in location_map else \"Other\"\n",
    "\n",
    "location_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_type = dto[\"Location\"].replace(location_mapped).fillna(\"Other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deadline (dataframe, deadline, activity=None, days=7):\n",
    "    date_condition = (dataframe[\"Begin\"] <= deadline) & (dataframe[\"Begin\"] > (deadline - pd.Timedelta(days=days)))\n",
    "\n",
    "    full_condition = date_condition & (dataframe[\"Activity\"]==activity) if activity is not None else date_condition\n",
    "\n",
    "    deadline_df = dataframe[full_condition][\"Begin\"]\n",
    "    return days - (deadline - deadline_df) / pd.Timedelta(days=1)\n",
    "\n",
    "def deadline_presure(dataframe, deadlines, activity=None, days=7):\n",
    "    return pd.Series(pd.concat([compute_deadline(dataframe, d, activity=activity, days=days) for d in deadlines]) if len(deadlines) > 0 else 0.0, dataframe.index).fillna(0)\n",
    "\n",
    "\n",
    "def compute_factor_deadline(dataframe, deadlines, days=7): \n",
    "    return pd.concat([deadline_presure(dataframe, deadlines[act], act, days=days) for act in deadlines], axis=1).sum(axis=1)\n",
    "\n",
    "def compute_factor_others(dataframe, deadlines, days=7):\n",
    "    deadline_df = pd.concat([deadline_presure(dataframe, deadlines[act], activity=None, days=days).rename(act + \" deadline\") for act in deadlines], axis=1)    \n",
    "    return pd.concat([deadline_df.drop(act+\" deadline\", axis=1).sum(axis=1)[dataframe[\"Activity\"]==act] for act in deadlines], axis=0)\n",
    "\n",
    "def compute_factor_trip(dataframe, trip_dates, days=7):\n",
    "    return deadline_presure(dataframe, deadlines=trip_dates, days=days)\n",
    "\n",
    "def compute_factor_weekend_work(dataframe):\n",
    "    return (dataframe[\"Begin\"].dt.dayofweek >= 5) * 1.0\n",
    "\n",
    "def compute_factor_summer_work(dataframe):\n",
    "    return ((dataframe[\"Begin\"].dt.month == 7) | ((dataframe[\"Begin\"].dt.month == 8) & (dataframe[\"Begin\"].dt.day < 15))) * 1.0\n",
    "\n",
    "def compute_location(projs, location_mapped):\n",
    "    loc_duration = projs[\"Duration\"].groupby([projs[\"Activity\"],  projs[\"Begin\"].dt.date, projs[\"Location\"].replace(location_mapped).fillna(\"Other\")]).sum()\n",
    "    all_duration = projs[\"Duration\"].groupby([projs[\"Activity\"],  projs[\"Begin\"].dt.date]).sum()\n",
    "    loc_duration = (loc_duration.unstack() / pd.Timedelta(\"1m\") )\n",
    "    loc_duration[\"all\"] = all_duration / pd.Timedelta(\"1m\")\n",
    "    return (loc_duration[\"Other\"].fillna(0) / loc_duration[\"all\"] * 0.5 + loc_duration[\"Work\"].fillna(0) / loc_duration[\"all\"]).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "def compute_percentage_interval(projs, begin_interval='12:30:00', end_interval='13:30:00'):\n",
    "    type_A = ((projs[\"End\"].dt.tz_localize(None).dt.time >= pd.to_datetime(end_interval).time()) & (projs[\"Begin\"].dt.tz_localize(None).dt.time <= pd.to_datetime(begin_interval).time())) * pd.Timedelta('1h') \n",
    "    type_B = ((projs[\"Begin\"].dt.tz_localize(None).dt.time >= pd.to_datetime(begin_interval).time()) & (projs[\"End\"].dt.tz_localize(None).dt.time <= pd.to_datetime(end_interval).time())) * projs[\"Duration\"]\n",
    "    type_C = ((projs[\"Begin\"].dt.tz_localize(None).dt.time < pd.to_datetime(begin_interval).time()) & (projs[\"End\"].dt.tz_localize(None).dt.time >= pd.to_datetime(begin_interval).time()) & (projs[\"End\"].dt.tz_localize(None).dt.time <= pd.to_datetime(end_interval).time())) * (projs[\"End\"].dt.tz_localize(None) - pd.to_datetime(projs[\"End\"].dt.date.astype(str) + \" \" + begin_interval))\n",
    "    type_D = ((projs[\"Begin\"].dt.tz_localize(None).dt.time >= pd.to_datetime(begin_interval).time()) & (projs[\"Begin\"].dt.tz_localize(None).dt.time <= pd.to_datetime(end_interval).time()) & (projs[\"End\"].dt.tz_localize(None).dt.time > pd.to_datetime(end_interval).time())) * (pd.to_datetime(projs[\"Begin\"].dt.date.astype(str) + \" \" + end_interval) - projs[\"Begin\"].dt.tz_localize(None))\n",
    "\n",
    "    duration_lunch = type_A + type_B + type_C + type_D\n",
    "    all_duration = projs[\"Duration\"].groupby([projs[\"Activity\"],  projs[\"Begin\"].dt.date]).sum()\n",
    "\n",
    "    return (duration_lunch.groupby([projs[\"Activity\"], projs[\"Begin\"].dt.date]).sum() / all_duration).reset_index().sort_values([\"Activity\", \"Begin\"])[0]\n",
    "\n",
    "def compute_factor_lunch_work(projs, begin_lunch='12:00:00', end_lunch='13:00:00'):\n",
    "    return compute_percentage_interval(projs, begin_lunch, end_lunch)\n",
    "\n",
    "def compute_factor_evening_work(projs, begin_evening='17:30:00', end_evening='23:59:59'):\n",
    "    return compute_percentage_interval(projs, begin_evening, end_evening)\n",
    "\n",
    "\n",
    "def compute_factor_calendar_activities(pdaily, calendar):\n",
    "    caldf = pd.DataFrame(calendar[\"Subject\"])\n",
    "    caldf[\"Start\"] = pd.to_datetime(calendar[\"Start Date\"] + \" \" + calendar[\"Start Time\"], format=\"%d-%m-%Y %H:%M:%S\").dt.tz_localize(\"Europe/Amsterdam\")\n",
    "    caldf[\"End\"] = pd.to_datetime(calendar[\"End Date\"] + \" \" + calendar[\"End Time\"], format=\"%d-%m-%Y %H:%M:%S\").dt.tz_localize(\"Europe/Amsterdam\")\n",
    "    events_day = caldf.groupby(caldf[\"Start\"].dt.date)[\"Subject\"].count()\n",
    "    events_day.index = pd.to_datetime(events_day.index).tz_localize(\"Europe/Amsterdam\")\n",
    "    return pd.merge(pdaily, events_day, how=\"left\", left_on=\"Begin\", right_index=True)[\"Subject\"].fillna(0)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pdaily[\"Begin\"], pdaily[\"Begin\"].dt.dayofweek, (pdaily[\"Begin\"].dt.dayofweek > 5) * 1.0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RCIS\n",
    "# 10 Feb 2023 submission\n",
    "# 31 Mar 2023 camera ready\n",
    "# 25 May 2023 presentation\n",
    "# 1 Oct 2023 extension\n",
    "\n",
    "\n",
    "# ICPM\n",
    "# 13 Jun 2023 (original deadline)\n",
    "# 20 Jun 2023 submission\n",
    "# 5 sep 2023 Camera ready\n",
    "# 24 oct 2023 presentation\n",
    "\n",
    "deadlines = {\n",
    "    \"RCIS paper\": [\n",
    "        pd.Timestamp(year=2023, month=2, day=11, tz=\"Europe/Amsterdam\"),\n",
    "        pd.Timestamp(year=2023, month=4, day=1, tz=\"Europe/Amsterdam\"),\n",
    "        pd.Timestamp(year=2023, month=5, day=25, tz=\"Europe/Amsterdam\"),\n",
    "        pd.Timestamp(year=2023, month=10, day=1, tz=\"Europe/Amsterdam\"),\n",
    "    ], \n",
    "\n",
    "    \"ICPM on AWT\": [\n",
    "        pd.Timestamp(year=2023, month=6, day=13, tz=\"Europe/Amsterdam\"),\n",
    "        pd.Timestamp(year=2023, month=6, day=21, tz=\"Europe/Amsterdam\"),\n",
    "        pd.Timestamp(year=2023, month=9, day=6, tz=\"Europe/Amsterdam\"),\n",
    "        pd.Timestamp(year=2023, month=10, day=24, tz=\"Europe/Amsterdam\")\n",
    "    ],\n",
    "\n",
    "    \"Grand challenges\": [],\n",
    "\n",
    "    \"Map metaphor\": []\n",
    "}\n",
    "\n",
    "factors = pd.concat([\n",
    "    pdaily[\"Begin\"], pdaily[\"Activity\"], \n",
    "    compute_factor_deadline(pdaily, deadlines).rename(\"ClosenessToDeadline\"), \n",
    "    compute_factor_others(pdaily, deadlines).rename(\"ClosenessToDeadlineOthers\"), \n",
    "    compute_factor_calendar_activities(pdaily, calendar_csv).rename(\"ScheduledActivities\"), \n",
    "    compute_factor_trip(pdaily, trip_dates).rename(\"ClosenessToTrip\"),\n",
    "    compute_location(projs, location_mapped).rename(\"VicinityOfOthers\"),\n",
    "    compute_factor_weekend_work(pdaily).rename(\"WeekendWork\"),\n",
    "    compute_factor_evening_work(projs).rename(\"EveningWork\"),\n",
    "    compute_factor_lunch_work(projs).rename(\"LunchWork\"), \n",
    "    compute_factor_summer_work(pdaily).rename(\"SummerWork\")],\n",
    "    axis=1)\n",
    "\n",
    "factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "\n",
    "It computes pearson correlation between metrics and factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_merged = pd.merge(daily_metrics, factors, on=[\"Activity\", \"Begin\"])\n",
    "all_merged.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "all_merged.drop([\"Activity\", \"Begin\"], axis=1).corr(method=lambda x, y: pearsonr(x, y)[0]).drop(index=np.delete(daily_metrics.columns, np.where(daily_metrics.columns=='ActivityVariety')), columns=np.append(factors.columns.values, 'ActivityVariety') , errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"P-values < 0.01\")\n",
    "(all_merged.drop([\"Activity\", \"Begin\"], axis=1).corr(method=lambda x, y: pearsonr(x, y)[1]) < 0.05).drop(index=np.delete(daily_metrics.columns, np.where(daily_metrics.columns=='ActivityVariety')), columns=np.append(factors.columns.values, 'ActivityVariety') , errors='ignore')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predmon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
